# NerdCarX Fase 2 - Docker Compose
#
# Gebruik:
#   docker compose up -d            # Start alle services
#   docker compose up -d --build    # Rebuild en start
#   docker compose logs -f          # Volg logs
#   docker compose down             # Stop alle services

services:
  # Ollama LLM - Ministral 14B
  ollama:
    image: ollama/ollama:latest
    container_name: nerdcarx-ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']  # GPU0 - RTX 4090
              capabilities: [gpu]
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_KV_CACHE_TYPE=q8_0
      - OLLAMA_KEEP_ALIVE=-1
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # Voxtral STT - Speech-to-Text via vLLM
  # Custom image met audio dependencies (soundfile, ffmpeg, librosa)
  voxtral:
    build: ./stt-voxtral/docker
    image: nerdcarx/vllm-voxtral:latest
    container_name: nerdcarx-voxtral
    ports:
      - "8150:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']  # GPU1 - RTX 5070 Ti
              capabilities: [gpu]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ipc: host
    # vLLM configuratie - officieel aanbevolen door Mistral
    # Bron: https://huggingface.co/mistralai/Voxtral-Mini-3B-2507#vllm-recommended
    command: >
      --model mistralai/Voxtral-Mini-3B-2507
      --tokenizer_mode mistral
      --config_format mistral
      --load_format mistral
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --gpu-memory-utilization 0.75
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 180s
    restart: unless-stopped

  # Fish Audio TTS - Text-to-Speech
  tts:
    image: fishaudio/fish-speech:latest
    container_name: nerdcarx-tts
    ports:
      - "8250:8080"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']  # GPU0 - RTX 4090
              capabilities: [gpu]
    volumes:
      - ./tts/fishaudio/checkpoints:/app/checkpoints:ro
      - ./tts/fishaudio/references:/app/references  # Niet ro, Fish Audio schrijft hier
      - tts-cache:/root/.cache  # Compile cache persisteren
    entrypoint: uv
    command: run tools/api_server.py --listen 0.0.0.0:8080 --compile
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  # Orchestrator - Main API
  orchestrator:
    build: ./orchestrator
    image: nerdcarx/orchestrator:latest
    container_name: nerdcarx-orchestrator
    ports:
      - "8200:8200"
    volumes:
      - ./config.yml:/app/config.yml:ro
    environment:
      - OLLAMA_URL=http://ollama:11434
      - VOXTRAL_URL=http://voxtral:8000
      - TTS_URL=http://tts:8080
    depends_on:
      ollama:
        condition: service_healthy
      voxtral:
        condition: service_healthy
      tts:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8200/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

volumes:
  ollama:
    external: true  # Gedeelde volume met andere ollama container
  tts-cache:
    name: nerdcarx-tts-cache  # Compile cache voor Fish Audio
